import streamlit as st
import os
import tempfile
import json
from agent.core import get_agent_executor
from agent.tools import set_current_segy_path
from langchain_core.messages import HumanMessage, AIMessage


def format_intermediate_steps(intermediate_steps):
    """Turn LangChain intermediate steps into markdown for display."""
    if not intermediate_steps:
        return "ï¼ˆæœ¬æ¬¡æ²¡æœ‰è°ƒç”¨å·¥å…·ï¼‰"

    blocks = []
    for idx, (action, observation) in enumerate(intermediate_steps, start=1):
        tool_input = action.tool_input
        if isinstance(tool_input, dict):
            tool_input_str = json.dumps(tool_input, ensure_ascii=False)
        else:
            tool_input_str = str(tool_input)

        block = (
            f"{idx}. **å·¥å…·** `{action.tool}`\n"
            f"   - è¾“å…¥: `{tool_input_str}`\n"
            f"   - è§‚å¯Ÿ: {observation}"
        )
        blocks.append(block)

    return "\n\n".join(blocks)

# Page Config
st.set_page_config(page_title="QuakeCore AI Agent", layout="wide")

st.title("ğŸŒ‹ QuakeCore AI - åœ°éœ‡æ•°æ®æ™ºèƒ½åŠ©æ‰‹")

# Initialize Session State
if "messages" not in st.session_state:
    st.session_state.messages = []

if "agent" not in st.session_state:
    st.session_state.agent = None

if "agent_config" not in st.session_state:
    st.session_state.agent_config = None

if "agent_error" not in st.session_state:
    st.session_state.agent_error = None

# Sidebar for Configuration
uploaded_file = None
local_file_path = None
with st.sidebar:
    st.header("æ¨¡å‹é…ç½®")
    provider_options = {
        "æœ¬åœ° Ollama": "ollama",
        "DeepSeek API": "deepseek",
    }
    provider_label = st.selectbox("é€‰æ‹©æ¨ç†å¼•æ“", list(provider_options.keys()), index=0, key="provider_select")
    provider = provider_options[provider_label]

    current_agent_config = {}
    if provider == "ollama":
        model_name = st.text_input("æœ¬åœ°æ¨¡å‹åç§° (Ollama)", value="qwen2.5:3b", key="ollama_model_input")
        st.info("è¯·ç¡®ä¿æœ¬åœ°å·²å®‰è£… Ollama å¹¶è¿è¡Œäº†å¯¹åº”æ¨¡å‹ (ä¾‹å¦‚: `ollama run qwen2.5:3b`)")
        current_agent_config = {
            "provider": "ollama",
            "model_name": model_name,
            "api_key": None,
            "base_url": None,
        }
    else:
        deepseek_model = st.text_input("DeepSeek æ¨¡å‹åç§°", value="deepseek-chat", key="deepseek_model_input")
        deepseek_api_key = st.text_input(
            "DeepSeek API Key",
            value=os.getenv("DEEPSEEK_API_KEY", ""),
            type="password",
            key="deepseek_api_key_input",
        )
        deepseek_base_url = st.text_input(
            "DeepSeek Base URL",
            value="https://api.deepseek.com",
            key="deepseek_base_url_input",
        )
        current_agent_config = {
            "provider": "deepseek",
            "model_name": deepseek_model,
            "api_key": deepseek_api_key,
            "base_url": deepseek_base_url,
        }
        st.info("ä½¿ç”¨ DeepSeek æ—¶éœ€è¦æœ‰æ•ˆçš„ API Keyï¼Œå¯åœ¨ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY ä¸­é…ç½®ã€‚")
    
    st.divider()
    st.header("æ•°æ®æº")
    data_source = st.radio("é€‰æ‹©æ•°æ®æ¥æº", ["ä¸Šä¼ æ–‡ä»¶", "æœ¬åœ°æµ‹è¯•æ–‡ä»¶"])
    
    uploaded_file = None
    local_file_path = None
    
    if data_source == "ä¸Šä¼ æ–‡ä»¶":
        uploaded_file = st.file_uploader("ä¸Šä¼  SEGY æ–‡ä»¶", type=["segy", "sgy"])
    else:
        local_file_path = st.text_input("æœ¬åœ°æ–‡ä»¶è·¯å¾„", value="data/viking_small.segy")
        if st.button("åŠ è½½æœ¬åœ°æ–‡ä»¶"):
            if os.path.exists(local_file_path):
                st.session_state.current_file_path = os.path.abspath(local_file_path)
                st.session_state.uploaded_filename = os.path.basename(local_file_path)
                set_current_segy_path(st.session_state.current_file_path)
                st.success(f"å·²åŠ è½½æœ¬åœ°æ–‡ä»¶: `{local_file_path}`")
            else:
                st.error(f"æ–‡ä»¶ä¸å­˜åœ¨: `{local_file_path}`")

config_changed = current_agent_config != st.session_state.agent_config
if config_changed:
    if current_agent_config["provider"] == "deepseek" and not current_agent_config.get("api_key"):
        st.session_state.agent = None
        st.session_state.agent_config = None
        st.session_state.agent_error = "DeepSeek æ¨¡å¼éœ€è¦æä¾› API Keyã€‚"
    else:
        try:
            st.session_state.agent = get_agent_executor(**current_agent_config)
            st.session_state.agent_config = current_agent_config
            st.session_state.agent_error = None
        except Exception as err:
            st.session_state.agent = None
            st.session_state.agent_config = None
            st.session_state.agent_error = str(err)

agent_error = st.session_state.agent_error
agent_ready = st.session_state.agent is not None

if agent_error:
    st.error(agent_error)
elif not agent_ready:
    st.info("è¯·åœ¨ä¾§è¾¹æ å®Œæˆæ¨¡å‹é…ç½®ä»¥å¯åŠ¨å¯¹è¯ã€‚")

# Handle File Upload
if uploaded_file:
    # Save uploaded file to a temporary location
    # In a real app, you might want to manage storage more persistently
    if "current_file_path" not in st.session_state or st.session_state.uploaded_filename != uploaded_file.name:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".segy") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name
        
        st.session_state.current_file_path = tmp_path
        st.session_state.uploaded_filename = uploaded_file.name
        
        # Add a system message indicating file is ready
        st.success(f"æ–‡ä»¶ `{uploaded_file.name}` å·²åŠ è½½ï¼Œä½ å¯ä»¥è¯¢é—®å…³äºå®ƒçš„é—®é¢˜äº†ï¼")

# Ensure the tool context is updated on every run if file exists
if "current_file_path" in st.session_state:
    set_current_segy_path(st.session_state.current_file_path)
    
    # Optional: Auto-trigger an analysis
    # st.session_state.messages.append({"role": "assistant", "content": "æˆ‘å·²åŠ è½½æ–‡ä»¶ã€‚ä½ å¯ä»¥é—®æˆ‘å®ƒçš„ç»“æ„ã€å¤´ä¿¡æ¯æˆ–æ•°æ®å†…å®¹ã€‚"})

# Chat Interface
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if steps := message.get("steps"):
            with st.expander("æ€è€ƒè¿‡ç¨‹", expanded=False):
                st.markdown(steps)

# User Input
prompt = st.chat_input(
    "è¾“å…¥ä½ çš„é—®é¢˜ (ä¾‹å¦‚: è¯»å–segyæ–‡ä»¶ï¼Œç»™æˆ‘è¯´æ˜å…¶å†…éƒ¨çš„ç»“æ„)",
    disabled=not agent_ready,
)

if prompt and agent_ready:
    # Display user message
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # Generate response
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("æ€è€ƒä¸­...")
        
        try:
            # Prepare chat history for LangChain
            chat_history = []
            for msg in st.session_state.messages[:-1]: # Exclude current prompt
                if msg["role"] == "user":
                    chat_history.append(HumanMessage(content=msg["content"]))
                else:
                    chat_history.append(AIMessage(content=msg["content"]))
            
            # Run Agent
            response = st.session_state.agent.invoke({
                "input": prompt,
                "chat_history": chat_history
            })
            
            answer = response["output"]
            steps_markdown = format_intermediate_steps(response.get("intermediate_steps", []))

            message_placeholder.markdown(answer)
            with st.expander("æ€è€ƒè¿‡ç¨‹", expanded=False):
                st.markdown(steps_markdown)

            st.session_state.messages.append({
                "role": "assistant",
                "content": answer,
                "steps": steps_markdown
            })
            
        except Exception as e:
            active_provider = (st.session_state.agent_config or current_agent_config or {}).get("provider", "ollama")
            provider_hint = "Ollama æœ¬åœ°æœåŠ¡" if active_provider == "ollama" else "DeepSeek API é…ç½®æˆ–ç½‘ç»œçŠ¶æ€"
            error_msg = f"å‘ç”Ÿé”™è¯¯: {str(e)}\n\nè¯·æ£€æŸ¥ {provider_hint}ã€‚"
            message_placeholder.error(error_msg)
            st.session_state.messages.append({"role": "assistant", "content": error_msg})

# Cleanup on exit (Optional - Streamlit handles temp files differently depending on OS, 
# but explicit cleanup is good practice if we were managing sessions manually)
